import argparse
import json
import os  # Added
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# The EarlyStopping class is no longer used, as the logic
# is now explicitly in _run_training_loop
# from utils.model_stuff import EarlyStopping
from utils.load_signals_teacher import PrepDataTeacher
from utils.prep_data_teacher import train_val_test_split_continual_t
from models.model import CNN_LSTM_Model

# --- Helper Functions ---

def _prepare_data(target, device, val_ratio, test_ratio=0.35, batch_size=32):
    """Loads, splits, and prepares data into DataLoaders."""
    with open('teacher_settings.json', 'r') as k:
        settings = json.load(k)
    
    ictal_X, ictal_y = PrepDataTeacher(target, 'ictal', settings).apply()
    interictal_X, interictal_y = PrepDataTeacher(target, 'interictal', settings).apply()

    # Use the updated split function to get train, val, and test sets
    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split_continual_t(
        ictal_X, ictal_y, interictal_X, interictal_y, 
        test_ratio=test_ratio, 
        val_ratio=val_ratio
    )

    # Create DataLoader for training
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) # Shuffle=True for training
    
    # Create DataLoader for validation
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)
    
    # Create tensors for testing
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)
    
    return train_loader, val_loader, X_test_tensor, y_test_tensor, X_train_tensor.shape

def _build_model_and_optimizer(input_shape, optimizer_type, device, lr=5e-4):
    """Initializes the model and its optimizer."""
    model = CNN_LSTM_Model(input_shape).to(device)
    if optimizer_type == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)
    else:  # Default to SGD
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    return model, optimizer

def _build_scheduler(optimizer):
    """Builds a learning rate scheduler."""
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', factor=0.1, patience=3
    )

def _train_epoch(model, data_loader, criterion, optimizer, device, clip_value):
    """Runs a single training epoch and returns the average loss."""
    model.train()
    total_loss = 0
    for X_batch, Y_batch in data_loader:
        optimizer.zero_grad()
        logits = model(X_batch)
        loss = criterion(logits, Y_batch)
        loss.backward()
        
        # --- Gradient Clipping ---
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def _validate_epoch(model, data_loader, criterion, device):
    """Runs a single validation epoch and returns the average loss."""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X_batch, Y_batch in data_loader:
            logits = model(X_batch)
            loss = criterion(logits, Y_batch)
            total_loss += loss.item()
    return total_loss / len(data_loader)

def _run_training_loop(model, train_loader, val_loader, optimizer, scheduler, 
                        epochs, patience, clip_value, device, pbar, model_save_path):
    """Executes the training process with validation and early stopping."""
    criterion = nn.CrossEntropyLoss()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(epochs):
        # --- Training ---
        avg_train_loss = _train_epoch(
            model, train_loader, criterion, optimizer, device, clip_value
        )
        train_losses.append(avg_train_loss)
        
        # --- Validation ---
        avg_val_loss = _validate_epoch(
            model, val_loader, criterion, device
        )
        val_losses.append(avg_val_loss)
        
        pbar.set_postfix({'train_loss': f'{avg_train_loss:.4f}', 
                         'val_loss': f'{avg_val_loss:.4f}'})
        pbar.update(1)
        
        # --- Scheduler Step ---
        scheduler.step(avg_val_loss)
        
        # --- Early Stopping Check & Model Saving ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            # Save the best model
            torch.save(model, model_save_path)
        else:
            epochs_no_improve += 1
        
        if epochs_no_improve >= patience:
            print(f"\nEarly stopping at epoch {epoch+1} with best val loss {best_val_loss:.4f}")
            break
            
    return train_losses, val_losses

def _evaluate_model(model, X_test, y_test):
    """Evaluates the trained model and returns the AUC score."""
    model.eval()
    with torch.no_grad():
        predictions = model(X_test)
    
    y_probs = F.softmax(predictions, dim=1)[:, 1].cpu().numpy()
    y_true = y_test.cpu().numpy()
    
    try:
        return roc_auc_score(y_true, y_probs)
    except ValueError:
        print("Warning: Could not compute AUC. Test set might be single-class.")
        return 0.5 # Default AUC

def _plot_losses(target, train_losses, val_losses):
    """Plots the training and validation losses."""
    plt.figure()
    plt.plot(train_losses, label=f'Patient {target} Train Loss')
    plt.plot(val_losses, label=f'Patient {target} Val Loss')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'Training & Validation Loss for Patient {target}')
    # plt.show() # Uncomment to display plots during execution

# --- Main Training Function ---

def train_teacher_model(target, epochs, optimizer_type, patience, val_ratio, clip_value):
    """
    Trains and evaluates a seizure detection model for a given patient.
    """
    print(f'\nTraining Teacher Model: Patient {target} | Epochs: {epochs} | Optimizer: {optimizer_type} | Patience: {patience}')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.cuda.empty_cache()

    # Create directory for models if it doesn't exist
    model_dir = 'pytorch_models'
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_path = os.path.join(model_dir, f'Patient_{target}_detection')

    # Prepare data, including validation loader
    train_loader, val_loader, X_test, y_test, shape = _prepare_data(
        target, device, val_ratio
    )
    
    teacher, optimizer = _build_model_and_optimizer(shape, optimizer_type, device)
    scheduler = _build_scheduler(optimizer)

    with tqdm(total=epochs, desc=f"Training Teacher for Patient {target}") as pbar:
        train_losses, val_losses = _run_training_loop(
            teacher, train_loader, val_loader, optimizer, scheduler, 
            epochs, patience, clip_value, device, pbar, model_path
        )
        
    # --- Load Best Model and Evaluate ---
    print(f"Loading best model from {model_path} for evaluation...")
    try:
        best_teacher = torch.load(model_path, weights_only=False).to(device)
        auc_test = _evaluate_model(best_teacher, X_test, y_test)
        print(f'Patient {target} - Best Model Test AUC: {auc_test:.4f}')
    except FileNotFoundError:
        print(f"Warning: No model saved at {model_path}. Evaluation skipped.")
        auc_test = 0.0 # Or np.nan

    # Plot both training and validation losses
    _plot_losses(target, train_losses, val_losses)
    
    return auc_test

# --- Execution Block ---

def main():
    """Main execution function to parse arguments and run training."""
    parser = argparse.ArgumentParser(description="Seizure Detection Model Training")
    parser.add_argument("--patient", type=str, required=True, help="Patient ID (or 'all')")
    parser.add_argument("--epochs", type=int, default=50, help="Max training epochs")
    parser.add_argument("--optimizer", type=str, choices=['SGD', 'Adam'], default='SGD', help="Optimizer")
    parser.add_argument("--patience", type=int, default=5, help="Early stopping patience")
    parser.add_argument("--val_ratio", type=float, default=0.2, help="Percentage of data for validation")
    parser.add_argument("--clip_value", type=float, default=1.0, help="Gradient clipping norm")
    
    args = parser.parse_args()

    default_patients = ['1', '2', '3', '5', '9', '10', '13', '18', '19', '20', '21', '23']
    patients_to_run = default_patients if args.patient == 'all' else [args.patient]
    
    results = {}
    for patient in patients_to_run:
        results[patient] = train_teacher_model(
            patient, args.epochs, args.optimizer, args.patience,
            args.val_ratio, args.clip_value
        )

    # Save results to a structured JSON file
    try:
        with open("Detection_results.json", 'r') as f:
            existing_results = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing_results = {}
    
    existing_results.update(results)
    with open("Detection_results.json", 'w') as f:
        json.dump(existing_results, f, indent=4)
        
    print("\nTraining complete. Results saved to Detection_results.json")

if __name__ == "__main__":
    main()